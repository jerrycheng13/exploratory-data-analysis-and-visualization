---
title: "Homework 3"
author: "Jinhan Cheng, jc4834@columbia.edu"
date: 2018/03/10
output:
  prettydoc::html_pretty:
    theme: tactile
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE,
                      cache = TRUE, echo = TRUE)
#library
library(tidyverse)
library(ggplot2)
library(data.table)
library(prettydoc)
library(parcoords)
library(GGally)
library(dplyr)
library(vcdExtra)
library(tidyquant)
library(extracat)
library(viridis)
library(skimr)
library(DT)
```

# **$1.$ Parallel Coordinates**

$(a)$ Draw a parallel coordinates plot of the data in "ManhattanCDResults.csv" in the data folder on CourseWorks. (Original data source and additional information about the data can be found here:  https://cbcny.org/research/nyc-resident-feedback-survey-community-district-results). Your plot should have one line for each of the twelve Manhattan community districts in the dataset. 

## Answer to Question 1(a):
```{r 1a, echo=TRUE}
dat1 <- t(read.csv(file = "ManhattanCDResults.csv"))
colnames(dat1) <- dat1[1,]
dat1 <- data.frame(dat1[3:14,])
dat1 <- data.frame(dat1) %>% rownames_to_column("cd")
dat1[,2:46] <- mutate_all(dat1[,2:46], function(x) as.numeric(sub("%", "", as.character(x))))/100
dat1$cd <- factor(dat1$cd, levels = dat1$cd)
dat1  %>% arrange(cd) %>%
  parcoords(
    rownames = F # turn off rownames from the data.frame
    , brushMode = "1D-axes"
    , reorderable = T
    , queue = T
    , color = list(colorBy = "cd", colorScale = htmlwidgets::JS("d3.scale.category10()")
    ) 
    , width = 11000
    , height = 400
  )
```

$(b)$ Do there appear to be greater differences across *indicators* or across *community districts*? (In other words, are Manhattan community districts more alike or more different in how their citizens express their satisfaction with city life? 

## Answer to Question 1(b):
* Yes, it appears that Manhattan community districts are more differnet in how their citizens express their satisfaction with city life. For example, we could see from the parcoords that the different scales within each indicator, which means that there lies a lot differences between each of them.

$(c)$ Which indicators have wide distributions (great variety) in responses?

## Answer to Question 3(c):
* Computing the standard deviation and using the globalminax scale, we could see that the following indicators have the **top ten** wide distributions: **Availability.of.cultural.activities**, **Crime.control**, **Cleanliness.of.your.neighborhood**, **Neighborhood.as.a.place.to.live**, **Neighborhood.parks**, **Neighborhood.playgrounds**, **Rat.control**, **Feeling.safe.walking.alone.in.your.neighborhood.at.night**, **Quality.of.life.in.NYC**, **Snow.removal**.
```{r 3c, echo=TRUE, fig.width = 10, fig.height = 25}
head(sort(apply(dat1[,2:46],2,sd),decreasing=TRUE),10)
ggparcoord(dat1, columns = 2:46, groupColumn = "cd", scale = "globalminmax") + coord_flip()
```

$(d)$ Does there appear to be a correlation between districts and overall satisfaction?  In order words, do some districts report high satisfaction on many indicators and some report low satisfaction on many indicators or are the results more mixed? (Hint: a different color for each community district helps identify these trends). 

## Answer to Question 1(d):
* We've already plot a new parcoord in Question 1(c) using globalminmax scale and draw each line with different colors. The results are more mixed. For some indicators, the lines tend to overlap, for some indicators, some districts report high satisfaction and some report low satisfaction, for example, those indicators with great variety **Availability.of.cultural.activities ** for example.

# **$2.$ Mosaic Plots**

Using the "Death2015.txt" data from the previous assignment, create a mosaic plot to identify whether `Age` is associated with `Place of Death`. Include only the top four `Age` categories. Treat `Age` as the independent variable and `Place of Death` as the dependent variable. (Hint: the dependent variable should be the last cut and it should be horizontal.) The labeling should be clear enough to identify what's what, that is, "good enough," not perfect. Do the variables appear to be associated? Describe briefly.

## Answer to Question 2:
* The lies huge difference between the each age group, and the variables appear to be associated. For example, with age increases, their is a tendency for percentage of **Nursing home/long term care** in **Place.of.Death** to increase.
```{r 2, echo=TRUE, fig.width = 18, fig.height = 15}
dat2 <- read.delim("Death2015.txt")
dat2 <- dat2 %>% filter(Ten.Year.Age.Groups %in% c("55-64 years", "65-74 years", "75-84 years", "85+ years"))
dat2$Ten.Year.Age.Groups <- as.factor(as.character(dat2$Ten.Year.Age.Groups))
dat2$Place.of.Death <- as.factor(as.character(dat2$Place.of.Death))
levels(dat2$Ten.Year.Age.Groups) <- c("55-64 years","65-74 years","75-84 years","85+ years")
dat2_t <- dat2[,c("Ten.Year.Age.Groups","Place.of.Death","Deaths")] %>% as.data.table
dat2_t <- dat2_t[,.(Freq=sum(Deaths)),by=c("Ten.Year.Age.Groups","Place.of.Death")]
vcd::mosaic(Place.of.Death ~ Ten.Year.Age.Groups, dat2_t, direction = c("v","h"), rot_labels = c(0,0,0,30))
```

# **$3.$ Time Series**

$(a)$ Use the `tidyquant` package to collect stock information on four stocks of your choosing.  Create a line chart showing the closing price of the four stocks on the same graph, employing a different color for each stock.

## Answer to Question 3(a):
```{r 3a, echo=TRUE}
apple <- tq_get("AAPL", get = "stock.prices", from = "2008-01-01", to = "2018-01-01")
facebook <- tq_get("FB", get = "stock.prices", from = "2008-01-01", to = "2018-01-01")
amazon <- tq_get("AMZN", get = "stock.prices", from = "2008-01-01", to = "2018-01-01")
google <- tq_get("GOOG", get = "stock.prices", from = "2008-01-01", to = "2018-01-01")
dat3_a <- bind_rows("apple" = apple, "facebook" = facebook, "amazon" = amazon, "google" = google, .id = "id")
dat3_a %>%
  ggplot(aes(x = date, y = close, color = id)) +
    geom_line(size = 1) +
    labs(title = "Apple, Facebook, Amazon and Google: Stock Prices") +
    theme_tq() + 
    scale_color_tq()
```

$(b)$ Transform the data so each stock begins at 100 and replot. Do you learn anything new that wasn't visible in part (a)?

## Answer to Question 3(b):
* From part 3(a), it's not easy to read the trends of each line. Looking at plots in part 3(b), we scale the data to let them have the same beginning, and we could read the trend of each line for example, the stock prices of **Amazon** has the largest slope while **Google** has the smallest slope, and each line's slope is increasing.
```{r 3b, echo=TRUE}
apple[,5] <- apple[,5]/as.numeric(apple[1,5])*100
facebook[,5] <- facebook[,5]/as.numeric(facebook[1,5])*100
amazon[,5] <- amazon[,5]/as.numeric(amazon[1,5])*100
google[,5] <- google[,5]/as.numeric(google[1,5])*100
dat3_b <- bind_rows("apple" = apple, "facebook" = facebook, "amazon" = amazon, "google" = google, .id = "id")
dat3_b %>%
  ggplot(aes(x = date, y = close, color = id)) +
    geom_line(size = 1) +
    labs(title = "Apple, Facebook, Amazon and Google: Stock Prices") +
    theme_tq() + 
    scale_color_tq()
```

# **$4.$ Missing Data**

For this question, explore the New York State Feb 2017 snow accumulation dataset available in the data folder on CourseWorks: "NY-snowfall-201702.csv". The original data source is here: https://www.ncdc.noaa.gov/snow-and-ice/daily-snow/

$(a)$ Show missing patterns graphically.

```{r 4a, echo=TRUE, fig.width=15, fig.height=40}
dat4 <- fread(input = "NY-snowfall-201702.csv")
dat4 <- dat4[,-c(1,3:6)]
tidydat4 <- dat4 %>% 
    gather(key, value, -`Station Name`) %>% 
    mutate(missing = ifelse(value=="M", "yes", "no"))
tidydat4$key <- as.factor(tidydat4$key)
levels(tidydat4$key) <- c("Feb 1","Feb 2","Feb 3","Feb 4","Feb 5","Feb 6","Feb 7","Feb 8","Feb 9","Feb 10","Feb 11","Feb 12","Feb 13","Feb 14","Feb 15","Feb 16","Feb 17","Feb 18","Feb 19","Feb 20","Feb 21","Feb 22","Feb 23","Feb 24","Feb 25","Feb 26","Feb 27","Feb 28")
ggplot(tidydat4, aes(x = key, y = fct_rev(`Station Name`), fill = missing)) +
  geom_tile(color = "white") + 
  ggtitle("NY-snowfall with NAs added") +
  scale_fill_viridis_d() + # discrete scale
  theme_bw()
```

$(b)$ Is the percent of missing values consistent across days of the month, or is there variety? 

## Answer to Question 4(b):
* The percent of missing values are not consistent across days of the month, and there is high variety.
```{r 4b, echo=TRUE}
dat4[dat4=="M"] <-NA
dat4[dat4=="T"] <-0.0025
skimr::skim(dat4) %>% filter(stat == "missing") %>%
  arrange(desc(value)) %>% select(variable, type, value) %>% mutate(percent = value/nrow(dat4)) %>% datatable()
visna(dat4)
```

$(c)$ Is the percent of missing values consistent across collection stations, or is there variety?

## Answer to Question 4(c):
* The percentage of missing values is not consistent across collection stations, there is high variety.
```{r 4c, echo=TRUE}
dat4_c <- data.frame(t(dat4))
dat4_c <- mutate_all(dat4_c, function(x)  as.character(x))
colnames(dat4_c) <- dat4_c[2,]
data.table(skimr::skim(dat4_c)) %>% filter(stat == "missing") %>%
  arrange(desc(value)) %>% select(variable, type, value) %>% mutate(percent = value/nrow(dat4_c)) %>% datatable()
visna(dat4_c)
```

$(d)$ Is the daily average snowfall correlated with the daily missing values percent?  On the basis of these results, what is your assessment of the reliability of the data to capture true snowfall patterns? In other words, based on what you've discovered, do you think that the missing data is highly problematic, or not?

## Answer to Question 4(d):
* It's correlated with the dailty missing values percent.
* Based on what I've discovered, I think that the missing data is highly problematic because when missing percent is high, the mean tends to be very small, and from another plot where we exchange the X and Y axis, it seems that mean reaches its max at around percent 0.18, we don't know whether it is in real condition or it is caused by certain percent of missing value. After all, it is not reliable for us to predict and make conclusions under too many missing value.
```{r 4d, echo=TRUE}
dat4_d1 <- data.frame(t(dat4))[7:34,]
dat4_d1 <- data.frame(dat4_d1) %>% rownames_to_column("variable")
dat4_d1[,2:350] <- mutate_all(dat4_d1[,2:350], function(x)  as.numeric(as.character(x)))
dat4_d1$mean <- rowMeans(dat4_d1[,2:350], na.rm = TRUE)
dat4_d2 <- skimr::skim(dat4) %>% filter(stat == "missing") %>%
  arrange(desc(value)) %>% select(variable, type, value) %>% mutate(percent = value/nrow(dat4))
dat4_d2 <- dat4_d2[1:28,]
dat4_d <- merge(dat4_d1[,c(1,351)],dat4_d2[,c(1,4)],keyby="variable")
dat4_d %>%
  ggplot(aes(x = mean, y = percent)) +
    geom_line(size = 1) +
    labs(title = "daily average snowfall and daily missing values percent") +
    theme_tq()
dat4_d %>%
  ggplot(aes(x = percent, y = mean)) +
    geom_line(size = 1) +
    labs(title = "daily average snowfall and daily missing values percent") +
    theme_tq()
```
